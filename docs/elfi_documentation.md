# ELFI: Event-sourcing Literate File Interpreter

> 完整文档合集

生成时间: 
2025-08-11 11:00:31

---

## 目录

### Part I: 设计文档
- [动机](#动机)
- [数据建模](#数据建模)
- [存储与同步](#存储与同步)
- [Weave API](#weave-api)
- [Tangle API](#tangle-api)
- [解释器](#解释器)

### Part II: 实现
- [实现概览](#实现概览)
- [解析器与格式](#解析器与格式)
- [核心逻辑](#核心逻辑)
- [命令行接口](#命令行接口)

### Part III: 示例
- [ELF 文件示例](#elf-文件示例)

---


# Part I: 设计文档


## 动机


### 1.1. 文学化编程（Literate Programming, LP）的承诺与困境

文学化编程（Literate Programming, LP）的核心理念是将程序逻辑的阐述与源代码本身交织在一起，其呈现顺序遵循人类的叙事逻辑而非编译器的执行顺序 。这一范式旨在通过将代码、数据和解释性文本融合成一个连贯的整体，来提升代码的可读性、可维护性以及研究的可复现性 。LP的实践者相信，强迫自己以清晰的散文形式解释程序逻辑，能够揭示设计中考虑不周的决策，从而提升软件质量 。

然而，尽管LP的理念极具吸引力，其在实践中却面临着诸多挑战。这些挑战包括陡峭的学习曲线、成熟工具链的缺乏、重构代码与文档的复杂性，以及在协作环境中维持统一作者风格的困难 。这些固有的障碍限制了LP的广泛应用，并催生了多种现代化的、虽不完全遵循其原始教条但继承其精神的工具生态。当前，LP的原则主要体现在三个主流领域，每个领域都有其独特的侧重点和技术权衡：

-   **Jupyter Notebooks**：专为探索性数据科学、机器学习和交互式计算而设计，强调代码执行与结果可视化的即时反馈。
-   **LaTeX**：在学术界和科学出版领域占据主导地位，其首要目标是实现最高质量的排版和复杂的数学公式呈现。
-   **Org-mode**：作为Emacs生态系统中的一个强大工具，广泛用于个人知识管理、任务规划和可复现研究，以其极高的可定制性和纯文本特性著称。

### 1.2. 协作鸿沟：对当前工具缺陷的系统性分析

尽管上述工具在各自的领域取得了巨大成功，但在多人协作，特别是异步协作和版本控制方面，它们普遍存在着深刻的结构性缺陷。这些缺陷构成了阻碍文学化编程成为真正高效的团队协作媒介的“协作鸿沟”。这些问题并非孤立的错误，而是源于一个共同的根本原因：这些工具的文件格式和核心数据模型是在单用户情境下设计的，协作功能是后来附加的，而非内建于其架构之中。

#### 1.2.1. Jupyter Notebooks：不透明且非确定性的状态之殇

Jupyter Notebook的根本问题在于其文件格式（.ipynb）是一个复杂的JSON对象，这与为纯文本设计的、基于行的版本控制系统（如Git）存在根本性的不兼容 。标准的`git diff`命令产生的输出几乎无法阅读，并且在面对大型notebook时常常失败 。`.ipynb`文件并非一个稳定的、人类可编辑的**源代码（source of truth）**，而是执行过程的**产物（artifact）**。它捕获了太多与逻辑无关的瞬时状态，这使得版本控制变得异常脆弱。

这种不兼容性导致了频繁且极具破坏性的合并冲突。这些冲突的根源往往是微不足道的差异，例如：

-   **单元格执行顺序**：`execution_count`元数据的不同会导致每个单元格都产生冲突 。
-   **非确定性输出**：代码输出中包含的内存地址、随机数或时间戳等易变信息，会在每次执行后都发生变化，从而引发冲突 。
-   **环境元数据**：内核名称、软件版本等环境信息的差异，即使在代码和输出完全相同的情况下，也会导致文件被标记为已修改 。

当合并冲突发生时，Git插入的冲突标记（`<<<<<<<`, `=======`, `>>>>>>>`）会破坏JSON结构，导致.ipynb文件无法被Jupyter打开，必须在文本编辑器中手动修复，这个过程极易出错 。为了缓解这一问题，社区开发了`nbdime`  和`nbdev`  等专业工具。然而，这些工具是外部依赖，需要所有协作者安装和配置，并未从根本上解决问题。它们更像是为一种有缺陷的格式打上的补丁，而不是一个原生的解决方案。

#### 1.2.2. LaTeX：中心化协作与分布式控制的二元对立

LaTeX的协作生态呈现出一种明显的两极分化，迫使用户在“易用但功能受限的中心化平台”和“强大但复杂的分布式VCS工作流”之间做出选择，没有一种方案能同时满足实时性、易用性和强大的版本控制需求。

一方面，以Overleaf为代表的在线平台提供了类似Google Docs的实时协作体验 。这种模式对用户友好，降低了协作门槛。但其本质是中心化的，通常伴随着商业限制，如协作者数量上限 。更重要的是，它将底层的版本控制抽象掉了，剥夺了开发者所期望的对提交、分支和合并的精细控制权。

另一方面，直接使用Git等版本控制系统管理`.tex`源文件，为开发者提供了强大的分布式工作流 。然而，这种方式缺乏实时反馈，且要求所有协作者都精通VCS操作。合并过程可能非常复杂，尤其是在文档结构或数学公式发生重大变化时，`latexdiff`等工具可能会生成无法编译的代码 。

#### 1.2.3. Org-mode：孤立天才的强大工具

Org-mode在个人知识管理和可复现研究方面功能极其强大，但其设计理念基本上是单用户的 。社区中普遍认为，在Org文件上进行原生、无缝的多人协作几乎是“不可能的”，需要付出巨大的额外努力 。

协作尝试通常依赖于外部的、非原生的同步机制，例如文件同步服务（如Dropbox）或版本控制系统（如Git） 。这些方案与LaTeX面临同样的问题，即合并是基于行的语法操作，而非语义操作。对外部同步的依赖导致了“知识库腐烂”（knowledge base rot）的现象，即事实的来源变得分散和不可靠，协作者无法确信他们看到的是最新或最准确的版本 。



| **特性**                 | **Jupyter Notebooks**                                        | **LaTeX (Overleaf)**                                        | **LaTeX (Git)**                                              | **Org-mode**                                              |
| ------------------------ | ------------------------------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
| **版本控制 (Git友好性)** | 差；不透明的JSON格式需要`nbdime`等专业工具才能进行有意义的diff/merge 。 | 不适用；版本历史由平台管理，对用户不透明，缺乏Git的灵活性。 | 优；纯文本格式，与Git原生兼容。                              | 优；纯文本格式，与Git原生兼容。                           |
| **实时并发编辑**         | 不支持；一次只能有一个用户安全地编辑文件。                   | 优；提供类似Google Docs的实时、无缝的协作体验 。            | 不支持；协作是异步的，通过pull/push操作进行。                | 不支持；依赖外部文件同步，非原生实时。                    |
| **合并冲突解决**         | 困难；冲突标记破坏JSON结构，手动解决风险高，且易因元数据产生伪冲突 。 | 自动；服务器通过“rebase”自动处理并发编辑，用户通常无感知 。 | 中等；基于行的文本合并，但`latexdiff`等工具在处理复杂结构时可能失败 。 | 困难；依赖Git进行文本合并，缺乏语义理解。                 |
| **元数据处理**           | 差；执行计数、内核信息等易变元数据与内容混杂，是冲突的主要来源 。 | 平台管理；用户无法直接控制。                                | 良好；元数据是文本的一部分，但不会自动变化。                 | 良好；元数据是文本的一部分，结构清晰。                    |
| **中心化程度**           | 分布式；文件本身是独立的，但协作通常依赖于中心化的Git仓库。  | 高度中心化；所有数据和协作逻辑都依赖于Overleaf服务器。      | 分布式；完全依赖于分布式的Git协议。                          | 分布式；文件独立，但协作依赖于中心化的同步服务或Git仓库。 |
| **可扩展性**             | 高；拥有庞大的扩展生态系统。                                 | 有限；功能由平台方提供。                                    | 高；可通过LaTeX宏包和外部脚本无限扩展。                      | 极高；Emacs Lisp提供了无与伦比的定制能力。                |

### 1.3. 结论：.elf 愿景——一种架构层面的解决方案

上述分析揭示了一个根本性的问题：现有工具的协作困境并非表面缺陷，而是深植于其核心架构的必然结果。它们将协作视为一个附加功能，试图嫁接到一个为单用户场景设计的、以静态文件为中心的世界观之上。这种架构上的不匹配是所有问题的根源。

为了从根本上解决这些问题，我们提出一种新的文学化编程数据格式—— **`.elf` (Event-Sourceable Literate File)**。`.elf`的设计理念是通过融合多个领域的先进思想，创建一个原生为协作而生的新范式。它代表了一种架构上的范式转变：不再将文档视为一个孤立的、静态的文件，而是将其视为一个活的、分布式的、可追溯的数据流。这种以事件为中心、协作为核心的设计，有望克服长期困扰文学化编程工具的障碍，使其真正成为现代软件工程和科学研究中强大、透明且高效的人机协作媒介。

其核心原则如下：

-   **解析器优先与人类可读**：`.elf`文件是一种具有明确语法的纯文本格式。这使得它在原始文本层面既对人类友好，也对版本控制系统友好。与此同时，它的结构可以被解析器精确地映射到一个丰富的、结构化的内存数据模型中。
-   **原生协作**：协作不是一个附加功能，而是内建于其核心数据模型中。通过使用无冲突复制数据类型（CRDTs），`.elf`从底层就支持并发和离线编辑，并能保证所有副本最终收敛到一致的状态。
-   **事件溯源**：文档不被视为一个静态的文件，而是被定义为一个可验证、可重放的所有修改操作的日志流。每一次击键、每一次代码单元的添加都是一个不可变的事件。这种模式为文档提供了终极的透明度、审计能力和强大的版本历史功能。
-   **去中心化与网络无关**：其底层架构支持从纯本地、点对-点（P2P）到客户端-服务器等多种网络拓扑和持久化策略。这使得用户可以摆脱对单一中心化服务器的依赖，实现真正的数据主权和灵活的部署。

基于这些开源思想和先进技术，我们已经着手实现了`.elf`格式的解释器内核——`elfi` (Event-sourcing Literate File Interpreter)。`elfi`通过灵活的元数据设计，能够优雅地平衡协作简易性与结构表达能力。接下来的文档将详细阐述`elfi`的各个架构层面的设计与实现蓝图。这份设计文档为后续的详细规格制定和原型开发奠定了坚实的理论和架构基础。


---


## 数据建模


### 2.1. 设计原则：事件溯源与CRDTs的融合

为了实现`.elf`原生协作和事件溯源的核心愿景，其数据模型必须从根本上区别于传统的文件格式。我们采纳了两项关键的设计原则：事件溯源（Event Sourcing）和无冲突复制数据类型（Conflict-Free Replicated Data Types, CRDTs）。

**事件溯源**是一种架构模式，其核心思想是系统的当前状态并非直接存储，而是通过重放一个不可变的、按时间顺序排列的事件日志来派生得出 。在`.elf`的语境下，这意味着文档的最终事实来源（Source of Truth）不是一个静态的文件快照，而是记录了从文档创建之初到当前时刻的每一次修改操作的完整日志。用户的每一个微小动作——无论是输入一个字符、创建一个代码块还是修改一个元数据标签——都直接映射为一个微小的、离散的操作事件 。这种模型为文档提供了终极的透明度、审计能力和强大的版本历史功能。

然而，在分布式协作环境中，简单地记录事件流是不够的。当多个用户在各自的设备上并发地产生事件时，如何保证这些分散的事件日志在合并后能形成一个全局一致的状态，就成了核心挑战。这正是**CRDTs**所要解决的问题。CRDTs是一类特殊的数据结构，它们允许在多个副本上进行并发、无需协调的更新，并保证这些副本在交换更新信息后，最终能收敛到完全相同的状态 [1, 2]。

CRDTs主要分为两大类：基于状态的（State-based, CvRDTs）和基于操作的（Operation-based, CmRDTs） [1, 3]。

-   **CvRDTs**通过传输和合并各自的完整状态来实现同步，设计相对简单，但当状态较大时网络开销会很高 。
-   **CmRDTs**则只传输引发状态变化的更新操作（例如，“在位置5插入字符'a'”），网络效率更高，但对通信层的可靠性有更高要求 。

`.elf`格式选择采用一种**基于操作的模型**，因为它与事件溯源的哲学完美契合：文档的当前状态仅仅是其完整操作历史的一个物化视图，而历史本身才是永恒的、不可篡改的真实记录 。

### 2.2. 技术选型：Automerge作为全历史CRDT实现

在确定了采用基于操作的CRDT和事件溯源原则后，我们选择了受**Automerge**项目启发的模型作为技术实现的基础 [1, 4]。选择Automerge而非其他CRDT实现（如Yjs）的关键决策点在于其对**完整操作历史的保留**。

Yjs为了追求极致的性能和内存效率，其设计默认包含了垃圾回收（GC）机制，会丢弃已删除内容的元数据和部分历史信息，这使得完整的历史追溯变得不可能 。这种设计优化了实时协作的性能，但牺牲了版本控制的深度。

相比之下，Automerge的核心特性是保留完整的、不可变的操作日志 。这个日志是文档的“DNA”，记录了其从诞生到当前状态的每一次演化。这种选择直接回应了Jupyter等工具在版本控制方面的核心痛点，为结构化、富媒体的文档带来了类似Git的强大版本管理能力。通过保留完整的操作日志，`.elf`为每个文档提供了全面的、可验证的演化历史，使得一系列高级功能成为可能 ：

-   **时间旅行**：可以检出（view）文档在任意历史时间点的状态。
-   **精确差异比较**：可以计算任意两个历史版本之间的精确差异（diff）。
-   **明确归因**：每一次变更都可以追溯到其作者和时间。

尽管历史上，保留完整历史的模型在性能和内存占用上存在劣势，但随着新一代实现的出现，通过在运行时也使用压缩的列式存储等技术，这一开销已得到显著降低，使得该模型在更广泛的场景中变得切实可行 。因此，`.elf`优先考虑数据的完整性、可审计性和版本控制能力，将Automerge模型作为其数据层的基石。

### 2.3. .elf文档结构：一个由块组成的CRDT

基于Automerge模型，`.elf`文档的整体结构被建模为一个行为类似JSON对象的CRDT，它本质上是一个从字符串键到CRDT值的映射表（Map CRDT） 。这种设计提供了高度的灵活性，允许在文档的顶层自由扩展，而无需预定义严格的模式。

文档的主要内容被组织在一个顶层键（例如，`"blocks"`）之下，该键对应的值是一个CRDT列表（List CRDT）。**至关重要的是，这个列表被设计为严格的扁平结构：列表中的元素只能是块（Block）对象，而块本身不能再包含其他的块列表。** 这种强制的扁平数据模型是一个核心的架构决策，旨在从根本上简化并发操作和冲突解决的复杂性，因为对扁平列表的操作（增、删、改、移）远比对嵌套树结构的操作更容易在CRDT中实现。这种结构借鉴了笔记本应用中“单元格”的概念，但为其赋予了强大的、原生的协作能力。文档的宏观结构——即块的顺序、增删和移动——都通过对这个CRDT列表的操作来完成。而如何在这种扁平结构之上表达丰富的层级关系，将在2.5节中详细阐述。

文档中的每个块本身也是一个CRDT映射表，这使得每个块都可以拥有自己的一组结构化属性。一个标准的`.elf`块包含以下核心属性：

-   `id`：一个全局唯一的、稳定的块标识符。这个ID由CRDT库在块创建时生成，确保即使在并发创建的情况下也能唯一识别每一个块。
-   `type`：一个字符串字面量，用于声明块的性质。例如，`"markdown"`表示一个散文块，`"code"`表示一个代码块。渲染层将根据此类型来决定如何展示和处理该块。
-   `content`：块的主要内容。对于基于文本的块（如markdown和code），其内容是一个**Text CRDT**实例。Text CRDT是一种专门为协同文本编辑优化的数据结构，它能够高效地处理并发的字符插入和删除，并正确地保留用户的编辑意图 。
-   `metadata`：一个嵌套的CRDT映射表，用于存储与块相关的任意元数据。这为块提供了极大的可扩展性，可以用于存储代码语言（如`{"language": "python"}`）、执行状态、作者信息、标签，以及用于构建层级结构的`parent`引用等。

### 2.4. 高级冲突解决：一种块级语义策略

标准的CRDT算法通过确定性规则（例如，比较操作的actorId）来自动解决并发写入，以保证所有副本最终收敛到一致的状态 。这种机制是“无冲突”的，因为它保证了数学上的一致性。然而，这种自动解决方式是**语义盲目**的。例如，当两个用户并发地修改同一个代码块时，简单地选择一个“胜利者”并丢弃另一个用户的修改，虽然保证了数据收敛，但却丢失了用户的意图，这在实践中是不可接受的 [5, 6]。

为了解决这个问题，`.elf`引入了一种更高级的、**块级（block-wise）的语义冲突解决机制**。该机制的核心思想是，冲突的解决方案不应是全局统一的，而应根据发生冲突的**块的类型**来选择最合适的策略。

#### 2.4.1. 策略分发机制

当底层CRDT引擎（如Automerge）检测到并发写入冲突时，它并不会立即丢弃“失败”的更新。相反，Automerge等实现提供了专门的API（如`getConflicts`）来访问这些被覆盖的冲突值 [1, 7]。`.elf`系统在此基础上构建了一个策略分发层。当对一个块的某个属性（如`content`）进行合并并检测到冲突时，系统会：

1.  检查该块的`type`字段。
2.  根据块类型，从一个策略注册表中查找并调用相应的冲突解决处理器。
3.  该处理器负责以一种语义上更合理的方式来处理冲突，而不是简单地依赖CRDT的默认行为。

这种设计将CRDT从一个纯粹的数据结构转变为一个可编程的、具备领域知识的协调引擎。

#### 2.4.2. 核心块类型的冲突解决策略

以下是为几种核心块类型设计的冲突解决策略示例：

-   **`type: "markdown"`**：对于Markdown文本块，其`content`是一个Text CRDT。Text CRDT本身已经能够很好地处理并发的字符级插入和删除，保留用户的编辑意图。因此，对于这类块，默认的Text CRDT合并行为通常是足够的，可以作为基线策略。
-   **`type: "code"`**：对于代码块，简单的文本合并是危险的，因为它很容易破坏代码的语法结构。这里的策略将更为复杂：
    1.  **尝试结构化合并**：当检测到`content`冲突时，处理器可以尝试对代码文本进行三路合并（3-way merge），类似于`git merge`的行为 [8]。如果可能，甚至可以利用语法树（AST）进行更智能的、语法感知的合并 [3]。
    2.  **标记冲突**：如果自动合并失败（例如，产生语法错误或无法解决的文本冲突），该策略**不会丢弃任何一方的修改**。相反，它会将块的内容设置为包含Git风格冲突标记（`<<<<<<<`, `=======`, `>>>>>>>`）的文本，并在块的`metadata`中设置一个冲突标志，例如`{"conflict": true}`。
    3.  **UI层呈现**：上层应用（如Weave或Tangle层）可以检测到这个元数据标志，并在UI中将该代码块高亮显示为“存在合并冲突”，并为用户提供工具来手动解决这些冲突。
-   **`type: "metadata"`**：对于元数据中的简单键值对（例如，两个用户同时将`language`从`"python"`修改为`"rust"`和`"go"`），标准的“最后写入者获胜”（LWW）规则通常是可接受的。然而，该框架也允许为特定的元数据键定义更复杂的策略，例如，对于一个表示“标签”的数组，其冲突解决策略可能是合并两个并发添加的标签，而不是选择其中一个。

这种块级策略使`.elf`系统变得高度可扩展。每当引入一种新的、具有复杂协作语义的块类型（例如，一个`spreadsheet`块或一个`kanban`块）时，开发者不仅需要定义其数据结构，还必须为其实现一个相应的冲突解决策略。这确保了系统的协作完整性可以随着其功能的丰富而同步增强。

### 2.5. 通过元数据实现层级结构 (Hierarchical Structures via Metadata)

#### 2.5.1. 问题陈述

纯粹的扁平块列表模型虽然极大地简化了协作的实现，但在表达复杂的、具有内在层级关系的文档结构（如书籍的章节、报告的大纲、嵌套的任务列表）时，其表达能力是有限的。在扁平列表中，所有块都处于同一层级，无法直观地体现它们之间的从属关系。

#### 2.5.2. 解决方案：邻接列表模型

为了在不牺牲协作简易性的前提下解决这个问题，`.elf`采用了一种在数据库设计中广泛使用的**邻接列表模型（Adjacency List Model）**。具体实现方式是，在每个块的`metadata`中引入一个可选的`parent`字段，其值是另一个块的`id`。

通过这种方式，我们在**不改变底层扁平数据模型（一个简单的CRDT列表）的前提下，在应用层逻辑中重建了树状的层级关系**。

#### 2.5.3. 协作优势

这种“用元数据模拟层级”的方案对于协作场景具有巨大的优势，完美地规避了在CRDT中直接实现树形结构的复杂性：

-   **低冲突的原子操作**：在传统的嵌套模型中，移动一个父块（例如，将整个章节从文档开头移到末尾）需要对大段文本进行操作，在并发编辑时极易产生难以解决的冲突。在我们的模型中，改变一个块的层级关系（例如，将一个小节移动到另一个章节下）仅仅是**修改该块`metadata`中`parent`字段的值**。这是一个极小的、原子性的元数据修改，CRDT处理起来非常轻松，冲突概率极低。
-   **职责分离**：该设计清晰地将“数据存储结构”与“逻辑呈现结构”解耦。
    -   **数据层 (`elfi-core`)**：只关心一个扁平的块列表和它们各自的属性。它的首要职责是保证数据同步的绝对可靠。
    -   **渲染层 (`Tangle`)**：负责读取块列表和它们的`parent`元数据，然后在UI上将它们渲染成用户看到的嵌套大纲视图。

这种架构决策让我们既能享受到扁平数据模型在协作上的巨大优势，又能为用户提供丰富的、层级化的视觉和交互体验。


---


## 存储与同步


### 3.1. 设计原则：将操作历史映射为消息流

在构建了一个强大的、为协作而生的数据模型之后，下一个关键挑战是设计一个高效、可靠且灵活的机制来持久化数据和同步更新。传统协作系统通常紧密耦合于特定的网络协议（如WebSocket）和后端架构（如中心化服务器），这限制了其部署的灵活性。

`.elf`的存储与同步架构基于一个核心的抽象原则：**将文档的CRDT操作历史记录（op log）映射为一个逻辑上的、持久化的消息发送流**。根据第二章的数据模型，文档的每一次变更都会生成一个或多个离散的、不可变的操作（operation）。我们可以将每一个这样的操作都看作是一条消息。因此，一个文档的完整历史就等同于一个按因果顺序排列的、包含了所有操作的消息队列或流。

这个简单的概念映射带来了巨大的架构优势。它将**数据模型（CRDT操作）**与**通信和存储（消息传递系统）**彻底解耦。一旦我们将问题重新定义为“如何可靠地传输、存储和查询一个命名的消息流”，我们就可以利用整个行业在消息队列、发布/订阅系统和分布式日志领域积累的成熟技术和解决方案，而无需重新发明轮子。这使得`.elf`的内核可以专注于CRDT逻辑，而将复杂的网络传输、路由、持久化和查询任务委托给一个专门的通信引擎。

### 3.2. 技术选型：Zenoh作为统一的数据中心网络

基于上述设计原则，我们选择了**Eclipse Zenoh**作为`.elf`底层的通信、存储和查询网络 。Zenoh并非一个简单的消息队列，而是一个集发布/订阅、分布式查询和存储于一体的统一数据中心协议，旨在统一处理动态数据（data in motion）、静态数据（data at rest）和计算 [1, 9]。Zenoh的架构特性与`.elf`的本地优先、去中心化协作的目标高度契合。

选择Zenoh的关键理由包括：

-   **拓扑无关性与去中心化**：Zenoh原生支持多种通信拓扑，包括点对点（P2P）、网状网络（Mesh）、路由模式（Routed）和代理模式（Brokered）。这意味着`.elf`文档可以在没有中心服务器的情况下，在局域网内的设备间直接同步；也可以通过Zenoh路由器跨越广域网和NAT，实现全球范围的协作。应用代码无需关心底层的网络拓扑。
-   **位置透明性**：在Zenoh网络中，数据生产者和消费者是解耦的。应用程序只需关心数据的“名字”（即Key Expression），而无需知道数据存储在哪里或由谁发布。Zenoh网络负责智能地路由数据和查询 。
-   **统一的API**：Zenoh通过一套统一的API来处理实时数据流（发布/订阅）和历史数据查询（查询/存储）。这完美地匹配了我们的核心抽象，即一个新加入的协作者既需要获取历史操作记录（数据在静止），也需要接收后续的实时更新（数据在运动）。
-   **高性能**：公开的性能评测显示，Zenoh在吞吐量和延迟方面显著优于MQTT、Kafka和DDS等主流协议，这对于提供流畅的实时协作体验至关重要 [10, 11, 12]。

### 3.3. 实现模型：Zenoh网络上的文档生命周期

`.elf`的数据模型与Zenoh的通信模型之间存在一种天然的、优雅的映射关系，这使得集成变得非常简单和高效。

-   **文档即键空间（Key-Space）**：每一个独立的`.elf`文档在Zenoh网络中被赋予一个唯一的键空间前缀，例如`/elf/docs/<doc_uuid>`。这个前缀成为了该文档在整个分布式系统中的唯一标识。
-   **操作即发布的消息**：当用户进行编辑时，由CRDT层生成的每一个原子操作（如第二章所述），都会被序列化并通过Zenoh**发布（PUT）**到该文档键空间下的一个特定子主题（或称通道）上，例如`/elf/docs/<doc_uuid>/ops`。
-   **事件流的实现**：因此，Zenoh通道上按时间顺序排列的消息流，就构成了该文档的完整、不可变的操作日志。
-   **实时同步**：已加入协作会话的客户端通过**订阅（SUBSCRIBE）**该主题来接收实时的更新操作。
-   **历史同步**：新加入的客户端或离线后重新上线的客户端，可以通过向该主题发起**查询（GET）**来获取所有历史操作记录。

这种设计将事件溯源的数据模型无缝地转化为一个发布/订阅/查询的通信模型。Zenoh的统一抽象使得实时同步（动态数据）和历史数据获取（静态数据）可以使用同一套API和底层机制来完成。一个新客户端只需对文档的键空间同时表达`GET`和`SUBSCRIBE`两种意图，Zenoh网络就会透明地处理从持久化存储中拉取历史数据，并转发后续的实时数据流，极大地简化了客户端的逻辑。

### 3.4. 通过Zenoh存储后端实现可插拔的持久化

`.elf`架构的一个核心优势在于将应用逻辑与底层的基础设施（存储和网络）彻底解耦，而Zenoh是实现这一目标的关键。Zenoh提供了一个强大的**存储后端（Storage Backend）插件机制** 。存储后端是一个特殊的Zenoh节点，它订阅特定的键空间，并将接收到的所有消息持久化到某种存储介质中。当有针对该键空间的查询请求时，存储后端会从介质中检索数据并返回。

这种机制为`.elf`带来了极大的灵活性：

-   **灵活的持久化选项**：通过为Zenoh路由器配置不同的存储后端插件，`.elf`文档的操作日志可以被持久化到任何地方，而无需修改`.elf`内核的任何代码。支持的存储选项包括：
    -   **文件系统后端**：将操作日志以文件形式存储在本地磁盘，适用于单机或简单的本地网络环境 。
    -   **RocksDB后端**：使用高性能的嵌入式键值数据库进行存储，适用于需要更高性能的服务器或桌面应用 。
    -   **InfluxDB后端**：适用于需要对操作历史进行复杂时序分析的场景 。
    -   **自定义后端**：可以通过Zenoh的API开发与其他任何数据库（如PostgreSQL, S3）集成的后端。
-   **真正的网络无关性**：`.elf`内核的职责仅限于向Zenoh网络发布和订阅CRDT操作。至于这些操作如何在网络中传输（TCP, UDP, QUIC）、如何路由、如何跨越NAT、以及最终如何被存储，完全由底层的Zenoh基础设施（由对等节点、路由器和存储节点组成）来决定和管理 。

这种架构使得`.elf`应用可以根据部署环境的需要，灵活地从一个完全离线的单机应用，无缝扩展为一个全球分布式的实时协作平台，而应用层的代码保持不变。


---


## Weave API


### 4.1. 设计原则：为内容创作者提供一个仓库模型

在建立了基于CRDT的数据模型和基于Zenoh的通信网络之后，我们需要一个高层次、对开发者友好的API来封装底层的复杂性。Weave层正是为此而设计。它面向文档内容的**主要贡献者**——作家、研究员、程序员和技术文档工程师——提供了一套用于编写和协作编辑文档“静态内容”（如文本和代码）的接口。

直接操作CRDT和发布/订阅协议对于应用开发者来说过于底层和复杂。因此，Weave层的核心设计原则是采用**仓库（Repository）模型**。该模型借鉴了`automerge-repo` [1, 13] 和Yjs Providers  等“全家桶”式协作框架的设计模式，将网络、存储和文档状态管理打包成一个易于使用的抽象层。

这种模型有效地将**文档管理**（发现、加载、同步）的关注点与**文档修改**的关注点分离开来。开发者通过一个简单的、面向对象的API与系统交互，而CRDT状态转换、序列化、网络消息收发等复杂机制则被完全封装。其核心组件包括：

-   **Repo（仓库）**：作为应用程序中管理协作状态的中心对象。它负责初始化和维护与Zenoh网络的连接，管理存储适配器，并持有一系列当前活跃的文档。
-   **DocHandle（文档句柄）**：作为单个`.elf`文档的引用或代理。开发者不直接持有文档对象本身，而是通过句柄来操作。句柄负责管理文档的整个生命周期，包括从网络或本地存储加载、处理进出的同步消息、触发更新事件，并提供修改文档的方法。

### 4.2. Weave API 规范

Weave API的设计旨在简洁和直观，同时充分利用底层全历史CRDT模型的强大能力，提供类似Git的版本控制功能和对层级结构的操作能力。

| **函数签名**                                              | **描述**                                                     |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| `repo.create() -> DocHandle`                              | 创建一个新的、空的`.elf`文档。此操作会在本地生成一个初始的CRDT状态，并为其分配一个唯一的URL。它会立即返回一个指向这个新文档的句柄。 |
| `repo.load(docUrl: String) -> DocHandle`                  | 根据一个唯一的文档URL（该URL映射到Zenoh的键空间）加载一个已存在的文档。此函数会返回一个句柄，并开始在后台异步地从网络和/或本地存储中获取文档的历史操作并构建其状态。 |
| `handle.change(callback: (doc: MutableDoc) => void)`      | 对文档进行所有修改的唯一入口。框架会将一个可变的、代理版本的文档状态传入`callback`中。开发者在此函数内部对代理对象进行的所有修改都会被记录下来，并在函数执行完毕后原子性地打包成CRDT操作广播出去。 |
| `handle.subscribe(callback: (doc: ImmutableDoc) => void)` | 为文档句柄注册一个监听器。每当该文档的状态因本地修改或接收到远程更新而发生变化时，注册的回调函数就会被调用，并传入新的、不可变的文档状态。这是UI层响应数据变化、进行重新渲染的主要机制。 |
| `handle.getHistory() -> HistoryGraph`                     | 返回构成文档历史的完整的、有向无环图（DAG）结构的操作日志。这使得高级的版本可视化和分析成为可能。 |
| `handle.viewAt(heads: ChangeHash) -> ImmutableDoc`        | 根据一组历史变更哈希，返回一个该文档在那个特定时间点的只读视图。这是实现“时间旅行”和版本回退功能的基础 。 |
| `handle.getParent(blockId: String) -> Option<Block>`      | 获取指定块的父块（如果存在）。                               |
| `handle.getChildren(blockId: String) -> Vec<Block>`       | 获取指定块的所有直接子块。                                   |
| `handle.getTree() -> Tree<Block>`                         | 将整个文档的扁平块列表重构为一个树状结构并返回。             |

新增的层级操作API（`getParent`, `getChildren`, `getTree`）封装了遍历CRDT列表并检查`parent`元数据字段的逻辑。它们为上层应用提供了一个自然的、面向树的编程模型，而无需关心底层的扁平存储实现。

### 4.3. 块级类型的Weave接口

为了提供更丰富和类型安全的编辑体验，Weave API并不会将所有块的内容都暴露为通用的文本。相反，在`handle.change`回调中访问块对象时，其`.content`属性会根据块的`type`提供不同的、专门化的API。

这种设计通过多态性为不同类型的内容提供了最自然的编辑原语，极大地提升了开发者的体验和代码的安全性。

-   **对于 `type: "markdown"` 或 `type: "code"` 的块**： 当开发者访问这类块的`content`属性时，他们会得到一个实现了**Text API**的对象。这个API提供了细粒度的文本操作方法，这些方法会直接映射到底层的Text CRDT操作。

    ```
    handle.change(doc => {
      const codeBlock = doc.blocks; // 假设这是一个code块
      //.content 提供了文本操作接口
      codeBlock.content.insert(0, "function hello() {\n");
      codeBlock.content.delete(20, 1); 
      codeBlock.content.insert(20, "}");
    });
    ```

-   **对于其他自定义类型的块**： 系统可以为其他类型的块提供专门的接口。例如，一个`type: "counter"`的块，其内容可能是一个CRDT计数器。

    ```
    handle.change(doc => {
      const counterBlock = doc.blocks; // 假设这是一个counter块
      //.content 提供了计数器操作接口
      counterBlock.content.increment(5);
    });
    ```

### 4.4. 在Weave层处理冲突

Weave API的设计旨在将底层CRDT的冲突信息清晰地暴露给应用程序，并提供一个明确的工作流来让用户参与语义冲突的解决。这与默默地丢弃冲突信息的模型形成了鲜明对比。

#### 4.4.1. 冲突发现API

Weave层将提供一个专门的API来查询特定属性上的并发写入冲突。

-   **`handle.getConflicts(path: Path) -> Map<OpId, Value> | undefined`**:
    -   **描述**: 此函数接收一个路径（`Path`），该路径指向文档状态树中的一个特定属性（例如 `['blocks', 0, 'content']`）。
    -   **返回值**: 如果该属性上存在由并发操作引起的多个值，函数将返回一个`Map`对象。这个`Map`的键是导致冲突的每个操作的唯一ID（`OpId`），值是该操作写入的具体值（`Value`）。如果该属性没有冲突，则返回`undefined`。
    -   **依据**: 这个API设计直接源于Automerge等全历史CRDT提供的能力，它们在合并时不会丢弃任何信息，而是保留所有并发写入的值，并通过类似`getConflicts`的接口供上层查询 [1, 7]。

#### 4.4.2. 冲突解决工作流

结合`getConflicts` API，应用程序可以实现一个强大的、用户驱动的冲突解决流程：

1.  **检测与呈现**：

    -   应用程序通过`handle.subscribe`监听文档的任何变化。
    -   在回调函数中，当检测到文档状态更新（特别是合并了远程更改后），应用程序可以主动调用`handle.getConflicts`来检查关键属性是否存在冲突。
    -   例如，在处理一个代码块时，UI代码可以检查`handle.getConflicts(['blocks', blockIndex, 'content'])`。
    -   如果返回了一个包含多个值的`Map`，UI层可以将这些不同的版本呈现给用户。例如，并排显示两个版本的代码，并询问用户：“Alice和Bob同时修改了这段代码，请选择要保留的版本，或手动合并它们。”

2.  **解决与提交**：

    -   一旦用户做出了选择（例如，选择了Bob的版本，或者手动编辑了一个合并后的新版本），应用程序将获取这个最终的、被认可的内容。
    -   然后，应用程序会发起一次**新的 `handle.change` 操作**，将这个最终内容写入到之前发生冲突的属性路径上。

    ```
    // 假设用户选择了Bob写入的值
    const finalContent = bobVersion; 
    
    handle.change(doc => {
      doc.blocks[blockIndex].content.replaceAll(finalContent);
    });
    ```

    -   这次新的`change`操作会在CRDT的因果历史中创建一个新的节点，该节点将所有先前冲突的“头”（heads）作为其父节点。这就在语义上解决了冲突，并确保所有协作者的文档状态都会收敛到这个由用户确认的、唯一的最终状态。

这个工作流清晰地划分了职责：CRDT负责在合并期间**无损地保存所有信息**；Weave API负责将这些冲突信息**透明地暴露给应用程序**；最终，应用程序和用户负责运用**领域知识和语义理解**来做出最终的裁决。


---


## Tangle API


### 5.1. 设计原则：通过选择性水合解耦状态与视图

如果说Weave层是为文档的“创作者”服务的，那么Tangle层就是为文档的“消费者”和“交互开发者”服务的。它的核心职责是将`.elf`文档中存储的静态数据（文本、代码）“水合”（hydrate）成一个动态的、可交互的、可执行的视图。

Tangle层的核心设计原则是**严格分离数据模型与UI视图**。UI（无论是DOM还是其他渲染目标）应被视为底层CRDT文档状态的一个纯粹的、反应式的**投影（projection）**，而不是事实的来源 。渲染过程遵循清晰的单向数据流：当Weave层通知有新的文档状态时，Tangle层会接收到这个新的、不可变的文档状态，然后通过协调（Reconciliation）过程，高效地将变化应用到UI上 。

至关重要的是，Tangle层必须解决现代富媒体文档的一个核心性能挑战：如何处理高频的UI交互事件（如拖动滑块、缩放图表）而不过度污染文档的持久化历史。传统的单页应用（SPA）模型通常将整个页面视为一个大的、有状态的JavaScript应用，这会导致初始加载时间长、可交互时间（Time to Interactive）延迟等问题 [14, 15]。为了避免这种情况，Tangle层采纳了**选择性水合（Partial Hydration）**的原则，即只为页面上真正需要交互性的部分加载和执行客户端JavaScript [16, 17]。

### 5.2. 技术选型：来自Slate.js和Lexical的灵感

Tangle层的架构深受Slate.js  和Lexical  等现代富文本编辑器框架的影响。这些框架的核心思想是，编辑器UI只是底层数据模型的一个渲染目标。它们通过定义一个与DOM解耦的、可序列化的`EditorState`对象来管理内容，并通过插件化的节点系统来支持自定义和嵌入式内容的渲染。例如，Lexical的`DecoratorNode`  或Slate.js的`void`元素  允许在文本流中嵌入任意的、由React等框架渲染的交互式组件。Tangle层将借鉴这种模式，将`.elf`文档中的每种块`type`映射到一个特定的渲染组件。

### 5.3. 更通用的基础：岛屿架构抽象

为了将选择性水合的原则提升到一个更通用和强大的架构层面，Tangle层的实现将基于**岛屿架构（Islands Architecture）** [18, 19, 20]。这一模式由Astro等现代Web框架推广，其核心思想是将网页视为一片静态HTML的“海洋”，其中点缀着若干个交互式的JavaScript“岛屿” [18, 21]。

#### 5.3.1. 岛屿架构在.elf中的应用

将岛屿架构应用于`.elf`文档的渲染，意味着Tangle层的渲染器会执行以下操作：

1.  **生成嵌套的静态海洋**：渲染器的核心职责之一是，在生成静态HTML“海洋”时，**必须读取所有块的`parent`元数据，并根据这些关系来构建DOM的嵌套结构**。它不再是渲染一个扁平的`<div>`列表，而是一个能够反映出章节和子章节关系的嵌套HTML结构（例如，使用嵌套的`<section>`或`<ul>`列表）。这部分构成了页面的“大陆”或“海洋”，可以被浏览器极快地解析和显示，并为UI层实现折叠/展开章节、面包屑导航等功能提供了结构基础。
2.  **定义交互岛屿**：对于需要交互性的块（例如，一个`type: "code"`且`metadata: {"interactive": true}`的块，或一个自定义的`type: "plotly-chart"`块），渲染器不会立即执行其逻辑。相反，它会在静态HTML中渲染一个占位符（例如一个带有特定`data-`属性的`<div>`），并标记这是一个需要“水合”的岛屿。
3.  **选择性水合**：客户端的JavaScript加载器会根据不同的策略来激活这些岛屿。例如，一个代码块的“运行”按钮可能在页面加载后立即水合（`client:load`），而一个位于文档末尾的数据可视化图表可能只有当用户滚动到它时才被水合（`client:visible`） [20]。每个岛屿都是一个独立的、自包含的应用（例如一个React或Vue组件），它只负责管理自己内部的逻辑和UI状态。

#### 5.3.2. 岛屿架构与CRDT的协同

岛屿架构天然地解决了交互组件之间状态管理复杂性的问题。在岛屿模型中，各个岛屿默认是相互隔离的 [22, 23]。这与`.elf`的架构完美契合：

-   **本地临时状态**：每个交互式岛屿（例如一个数据可视化组件）可以自由管理其内部的、临时的UI状态（如缩放级别、高亮选项）。这些状态是客户端独有的，不会自动同步或持久化，从而避免了因高频UI事件（如鼠标移动）而产生大量的CRDT操作。
-   **全局持久状态**：`.elf`文档的CRDT本身充当了所有岛屿共享的、唯一的**全局状态总线**。当一个岛屿需要将一个**有意义的、需要被持久化和协作共享**的状态变更提交时，它不会直接与其他岛屿通信，而是通过Tangle API调用一个特定的函数（见下文的`pinBlockState`），将该状态变更写入到全局的CRDT文档中。
-   **单向数据流**：其他客户端（或其他岛屿）通过订阅CRDT文档的变化来被动地接收这个状态更新，并以只读的方式反应在自己的视图中。

这种模式在瞬时UI状态和持久化协作状态之间建立了一道清晰、健壮且高性能的边界，是Tangle层架构的核心创新。

### 5.4. Tangle API 规范

Tangle API为渲染客户端（如Web前端）提供了一个更高层次的、面向状态的抽象，隐藏了Weave层的复杂性。

| **函数签名**                                                 | **描述**                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| `getDocumentState(docUrl: String) -> Promise<RenderableStateJSON>` | 高效地获取文档当前物化状态的一个可序列化、用于渲染的只读快照。返回的格式是易于前端框架消费的结构化对象（例如，一个与Lexical的`EditorState`兼容的JSON结构 ）。 |
| `pinBlockState(docUrl: String, blockId: String, metadataPatch: Object) -> Promise<void>` | **“钉合”（Pinning）机制**。这是Tangle层的核心API，允许一个交互式岛屿将其重要的内部状态变更提交回CRDT模型。它在底层会触发一次Weave层的`change()`操作，将`metadataPatch`合并到对应`blockId`的`metadata`属性中。例如，一个滑块组件可以在用户停止拖动时调用此函数，将最终的数值“钉合”到文档历史中。 |
| `executeCodeBlock(docUrl: String, blockId: String) -> AsyncStream<ExecutionEvent>` | 请求执行一个代码块。内核负责将代码和上下文路由到相应的沙箱化执行引擎（如通过Jupyter协议连接的Python内核），并将执行结果（如`stdout`, `stderr`, `display_data`等）以事件流的形式异步返回给客户端进行渲染。 |


---


## 解释器


### 6.1. 项目概述：elfi——一个基于Rust的无头内核

`.elf`系统的核心引擎被定义为一个名为`elfi`（Event-sourcing Literate File Interpreter）的**无头（headless）内核**。它是一个独立的系统，封装了文档的解析、协作、持久化和执行的核心逻辑，并通过定义良好的API与各种前端和工具进行交互 。之所以选择**Rust**作为实现语言，是基于对性能、内存安全以及生态系统协同性的战略考量。我们依赖的核心组件——Automerge（CRDT实现）、Zenoh（网络同步）和Tree-sitter（解析）——都拥有高质量的、性能卓越的Rust核心库 [1, 4, 24, 25]。将`elfi`本身定义为一个Rust项目，可以最大限度地减少跨语言调用的开销，确保整个系统的健壮性和效率。

`elfi`内核的设计是模块化的，可以被编译为多种目标产物：

-   一个**静态库（`lib.rs`）**，可以被其他Rust应用或通过FFI（Foreign Function Interface）被其他语言（如Python, C#, Swift）的应用嵌入。
-   一个**命令行工具（`main.rs`）**，用于对`.elf`文件进行离线操作，如验证、转换、合并等。
-   一个**WebAssembly（Wasm）模块**，使其核心逻辑能够在浏览器或其他Wasm兼容环境中运行。

### 6.2. 核心依赖与集成策略

`elfi`内核的实现将建立在三个 foundational Rust crates 之上，它们共同构成了系统的技术基石。

-   **解析 (`tree-sitter`)**: 内核将使用`tree-sitter`的Rust绑定 [26] 来处理`.elf`的纯文本格式。我们将为`.elf`设计一个明确的语法，并利用Tree-sitter的工具链来生成一个高效且容错的解析器。当内核加载一个`.elf`文件时，Tree-sitter解析器首先将其转换为一个抽象语法树（AST）。随后，内核会遍历这个AST，根据树的结构来初始化第二章中定义的Automerge CRDT文档模型，**这包括正确解析并设置每个块的`metadata`（含`parent`字段）**。
-   **数据模型 (`automerge`)**: 内核将直接嵌入官方的`automerge` Rust crate [4, 27] 作为所有文档在内存中的权威表示。所有对文档的修改、合并、历史查询等操作都将通过`automerge`库的API来完成。内核负责将来自网络的CRDT操作应用到内存中的文档实例，并将本地产生的更改序列化以供网络层发送。
-   **同步 (`zenoh`)**: 内核将集成官方的`zenoh` Rust crate [24, 28] 来处理所有的网络通信和存储交互。内核的配置将包括Zenoh的配置，例如连接的路由器地址、选择的持久化后端等。它将使用Zenoh的API来发布本地生成的CRDT操作，并订阅来自远程协作者的操作流。

### 6.3. 项目结构：一个模块化的Cargo工作空间

为了有效管理这个多方面系统的复杂性，并遵循Rust社区的最佳实践 [29]，`elfi`项目将被组织成一个**Cargo工作空间（workspace）**，其根目录包含一个虚拟清单（virtual manifest） [30, 31]。这种结构允许多个相关的crate共享同一个`Cargo.lock`文件和`target`目录，确保了依赖的一致性和编译效率。

| **Crate 名称** | **描述**                                                     | **主要依赖**                                              |
| -------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
| `elfi-core`    | 核心库crate，包含所有核心逻辑、数据结构，并暴露公共的Weave和Tangle API。这是整个系统的核心。 | `automerge`, `zenoh`, `elfi-parser`, `thiserror`, `tokio` |
| `elfi-parser`  | 一个专门的库crate，包含为`.elf`格式编写的Tree-sitter语法，以及将文本解析为AST并将其转换为初始CRDT状态的逻辑。 | `tree-sitter`                                             |
| `elfi-cli`     | 一个二进制crate，提供一个命令行工具，用于离线检查、创建、转换和管理`.elf`文件。 | `elfi-core`, `clap`, `anyhow`                             |
| `elfi-ffi`     | 一个`cdylib` crate，为`elfi-core`暴露一个C兼容的外部函数接口（FFI）。这为绑定到其他语言（如Python, Node.js）或编译到WebAssembly提供了基础。 | `elfi-core`                                               |

### 6.4. `elfi-core` 的核心接口与模块

`elfi-core` crate是整个系统的核心，其内部模块结构和暴露的公共API经过精心设计，以清晰地分离关注点。

#### 6.4.1. 模块结构

```
// elfi-core/src/lib.rs

// 公共API模块
pub mod doc;      // 对应Weave层API，用于内容创作和协作
pub mod render;   // 对应Tangle层API，用于渲染和执行
pub mod error;    // 定义库的公共错误类型

// 内部实现模块
mod sync;     // 管理Zenoh会话和网络同步逻辑
mod store;    // 管理内存中的Automerge文档实例集合
mod exec;     // 管理代码执行环境的交互（如Jupyter协议）
```

#### 6.4.2. 暴露的接口

-   **Weave API (`pub mod doc`)**:

    -   这个模块将暴露与第四章定义的Weave API相对应的Rust结构体和方法。
    -   `pub struct Repo`: 管理与Zenoh网络的连接和所有文档句柄。
    -   `pub struct DocHandle`: 单个文档的句柄，提供`change`, `subscribe`, `get_history`等方法。
    -   这个API是为需要深度集成、直接读写文档内容的工具（如IDE插件、原生桌面编辑器）设计的。

-   **Tangle API (`pub mod render`)**:

    -   这个模块将暴露与第五章定义的Tangle API相对应的函数，主要面向需要渲染交互式视图的前端或UI客户端。
    -   `pub async fn get_document_state(doc_url: &str) -> Result<serde_json::Value, ElfiError>`: 获取文档的JSON渲染快照。
    -   `pub async fn pin_block_state(doc_url: &str, block_id: &str, metadata_patch: serde_json::Value) -> Result<(), ElfiError>`: 实现“钉合”机制。
    -   `pub fn execute_code_block(doc_url: &str, block_id: &str) -> impl Stream<Item = Result<ExecutionEvent, ElfiError>>`:
        -   此函数将与`exec`模块交互，该模块负责管理与外部代码执行内核的通信。
        -   `exec`模块将使用一个现有的Jupyter协议客户端库（如`jupyter-protocol` [32, 33]）来连接到一个标准的Jupyter内核（如IPython）。
        -   它将代码块的内容通过Jupyter消息协议发送给内核执行，并将内核返回的输出（`stdout`, `display_data`等）封装成`ExecutionEvent`枚举，通过异步流返回给调用者。

-   **错误处理 (`pub mod error`)**:

    -   这个模块将定义整个库的公共错误类型。遵循Rust库设计的最佳实践，我们将使用`thiserror` crate来创建一个结构化的、详尽的`enum ElfiError` [34, 35]。

    ```
    use thiserror::Error;
    
    #
    pub enum ElfiError {
        #[error("Network error: {0}")]
        Network(#[from] zenoh::Error),
    
        #
        DataModel(#[from] automerge::AutomergeError),
    
        #[error("Parsing error: {0}")]
        Parse(String), // 来自elfi-parser的错误
    
        #
        DocumentNotFound(String),
    
        #[error("Execution error: {0}")]
        Execution(String),
    }
    ```

    -   这种设计为`elfi-core`的消费者提供了强大的能力，使其可以编程方式地匹配和处理不同类型的故障，这对于构建健壮的应用程序至关重要。与之相对，`elfi-cli`则会使用`anyhow`来包装来自`elfi-core`的`ElfiError`，并添加上下文信息，以便向最终用户提供清晰的错误报告 [36]。

---


# Part II: 实现


## 实现概览


本章节将前几章中高层次的架构原则，转化为 `elfi` Rust 内核更具体、更可操作的实现计划。

这些文档旨在成为“活文档”，将随着实施的进展而演变。它们作为开发人员的指南，概述了构成系统基础的核心数据结构、工作流程和 API 契约。

以下文档详细说明了 `elfi` 工作区中每个主要组件的实现细节：

- **[01 - 解析器 (`elfi-parser`)](./01-parser_and_format.md)**：定义了 `.elf` 文件的初始纯文本格式以及将其解析为 CRDT 模型的策略。

- **[02 - 核心逻辑 (`elfi-core`)](./02-core_logic.md)**：详细介绍了核心库的内部数据结构、异步工作流程和 API 实现。

- **[03 - 命令行界面 (`elfi-cli`)](./03-cli.md)**：规定了命令行工具的命令、参数和功能。

---


## 解析器与格式


本文档规定了 `.elf` 文件的初始纯文本格式，以及将此格式解析为基于 CRDT 的内存模型的策略。

### 1. `.elf` 纯文本格式

该格式旨在实现人类可读、对版本控制友好且易于解析。它由一系列**块 (Blocks)** 组成，块之间由标准 `---` 分隔符隔开。此设计直接受到所提供的 `example.elf.md` 的启发。

一个**块**有两个不同的部分：
1.  **元数据部分** (YAML Frontmatter)
2.  **内容部分**

#### 块结构示例：

```
---
id: block-C
type: code
metadata:
  parent: block-B
  language: python
---
import pandas as pd
penguins = sns.load_dataset("penguins")
penguins.head()
```

#### 元数据部分

-   **语法**：一个有效的 YAML 对象，由 `---` 包围，位于块的开头。
-   **必填字段**：
    -   `id` (String)：块的唯一标识符。虽然任何字符串都有效，但建议为新块使用 UUID 以防止冲突。
    -   `type` (String)：定义块类型的字符串字面量（例如 `"markdown"`, `"code"`）。这决定了内容如何被渲染和处理。
-   **可选字段**：
    -   `metadata` (Object)：一个用于存储任意元数据的嵌套 YAML 对象。这为可扩展性提供了命名空间，并且是存储语义信息（如以下内容）的必需位置：
        -   `parent` (String)：父块的 `id`，用于构建逻辑层级。
        -   `language` (String)：`code` 块的语言（例如 `"python"`, `"rust"`）。
        -   `interactive` (Boolean)：一个供 Tangle 使用的标志，以确定一个块是否应作为交互式孤岛被“激活”。

#### 内容部分

内容部分包含块的原始文本。它紧跟在元数据部分的结束 `---` 之后，并一直延伸到下一个块分隔符 (`---`) 或文件末尾。

### 2. 解析策略 (`elfi-parser`)

`elfi-parser` crate 负责将 `.elf` 文本格式转换为 `automerge` 文档实例。

#### 2.1. Tree-sitter 语法

将为 `tree-sitter` 创建一个 `grammar.js` 文件。它将定义以下结构：
-   一个 `source_file` 由一个或多个 `block` 节点组成。
-   一个 `block` 节点由一个 `metadata_section` 和一个 `content_section` 组成。
-   `metadata_section` 将被识别为由 `---` 包围的文本。
-   `content_section` 是剩余的文本。

这种方法可以高效且容错地将文件解析为具体语法树 (CST)。

#### 2.2. CST 到 CRDT 的转换

解析器将提供一个主函数：
`pub fn parse_to_doc(text: &str) -> Result<automerge::AutoCommit, ElfiError>`

转换过程如下：
1.  输入的 `text` 被 Tree-sitter 解析为 CST。
2.  解析器遍历 CST 的顶级 `block` 节点。
3.  对于每个 `block` 节点：
    a. 提取其 `metadata_section` 子节点的文本内容，并使用 YAML 解析器（例如 `serde_yaml`）进行解析。
    b. 在一个新的 `automerge` 事务中创建一个新的 map 对象，代表该块。
    c. 将解析出的 YAML 字段（`id`, `type`, `metadata`）插入到 Automerge map 中。
    d. `content` 字段被创建为一个 `automerge::Text` 对象，并使用 `content_section` 子节点的文本进行初始化。
    e. 这个新的块 map 被附加到 Automerge 文档中的顶级 `blocks` 列表中。
4.  返回最终填充好的 `automerge::AutoCommit` 文档。

---


## 核心逻辑


本文档详细介绍了 `elfi-core` 库的内部数据结构和工作流程，该库负责协调数据模型、同步和 API 层。

### 1. 核心数据结构

为了在 `async` 环境中有效管理并发和状态，我们将使用 `tokio` 和几种并发数据结构。

#### `Repo` 结构体

`Repo` 是应用程序的主要入口点。它管理网络会话和所有活动的文档句柄。

```rust
use dashmap::DashMap;
use std::sync::Arc;
use zenoh::Session;

pub struct Repo {
    zenoh_session: Arc<Session>,
    doc_handles: DashMap<String, Arc<DocHandle>>,
}
```
- `zenoh_session`：一个共享的 Zenoh 会话引用，用于所有网络操作。
- `doc_handles`：一个线程安全的哈希映射 (`DashMap`)，将文档的 URL（作为字符串）映射到其对应的 `DocHandle`。

#### `DocHandle` 结构体

`DocHandle` 管理单个文档的状态和同步。

```rust
use std::sync::Arc;
use tokio::sync::{Mutex, broadcast};
use automerge::AutoCommit;

pub struct DocHandle {
    doc_url: String,
    doc: Arc<Mutex<AutoCommit>>,
    subscribers: broadcast::Sender<()>, // 通知监听器任何变更
}
```
- `doc_url`：文档的唯一标识符。
- `doc`：文档的内存表示，包裹在 `tokio::sync::Mutex` 中以确保安全的并发访问。
- `subscribers`：一个 `tokio::sync::broadcast` 通道。每当文档状态发生变化时（无论是本地编辑还是远程更新），都会在此通道上发送一条消息。UI 层将订阅此通道的接收端。

### 2. 核心工作流程

#### 本地变更工作流程 (`handle.change()`)

此工作流程描述了当用户修改文档时发生的情况。

1.  **获取锁**：应用程序调用 `handle.change()`。第一步是异步获取 `DocHandle` 的 `Mutex<AutoCommit>` 上的锁。
2.  **执行闭包**：执行用户提供的闭包。它接收一个对 `AutoCommit` 文档的可变引用，并执行其编辑（例如，插入文本、更改元数据）。
3.  **获取变更**：当闭包完成且锁被释放后，我们调用 `doc.get_last_local_changes()` 来检索刚刚进行的二进制编码的变更。
4.  **本地广播**：调用 `subscribers.send(())` 以立即通知任何本地监听器（如 UI）文档已更新。
5.  **远程发布**：将二进制变更发布到适当的 Zenoh 键空间（例如 `/elfi/docs/{doc_url}/ops`），供远程对等方接收。

#### 远程变更工作流程 (Zenoh 订阅任务)

每个 `DocHandle` 将生成一个专用的 `tokio` 任务来监听来自网络的传入变更。

1.  **订阅**：该任务订阅文档的 Zenoh 键空间。
2.  **监听**：它异步等待传入的消息。
3.  **获取锁**：当收到包含远程变更的消息时，该任务获取 `DocHandle` 的 `Mutex<AutoCommit>` 上的锁。
4.  **应用变更**：使用 `doc.load_incremental(&remote_changes)` 将变更应用到本地模型。
5.  **本地广播**：调用 `subscribers.send(())` 以通知本地监听器更新，从而触发 UI 刷新。

### 3. Tangle API JSON 结构 (`get_document_state`)

为了便于在解耦的 UI 中进行渲染，`get_document_state` 函数将返回一个具有可预测的、受 Lexical 启发的结构的 JSON 对象。

```json
{
  "root_block_ids": ["block-A", "block-D"], // 顶级块 (父级为 null)
  "blocks": {
    "block-A": {
      "id": "block-A",
      "type": "markdown",
      "content": "# 分析帕尔默群岛企鹅数据...",
      "parent": null,
      "children": ["block-B"]
    },
    "block-B": {
      "id": "block-B",
      "type": "markdown",
      "content": "## 1. 数据加载...",
      "parent": "block-A",
      "children": ["block-C"]
    },
    "block-C": {
      "id": "block-C",
      "type": "code",
      "content": "import pandas as pd...",
      "parent": "block-B",
      "children": [],
      "metadata": {
        "language": "python"
      }
    }
    // ... 其他块
  }
}
```
这种结构允许 UI 框架轻松地重建文档的层级结构，并按 ID 访问每个块的内容和元数据。

---


## 命令行接口


本文档规定了 `elfi-cli` 二进制 crate 所需的命令、参数和功能。CLI 作为离线文件管理、验证和转换的工具。

我们将使用 `clap` crate 及其 “derive” 功能来解析参数。

### 命令

#### `elfi new <PATH>`

在指定路径创建一个新的、最小化的 `.elf` 文件。

-   **参数**：
    -   `<PATH>`：应创建新文件的路径。
-   **行为**：
    -   生成一个单一的、空的 `markdown` 块。
    -   为该块的 `id` 分配一个新的 `UUID`。
    -   将生成的文本写入指定文件。

#### `elfi validate <PATH>`

验证 `.elf` 文件的结构和语法。

-   **参数**：
    -   `<PATH>`：要验证的 `.elf` 文件的路径。
-   **行为**：
    -   读取文件内容。
    -   通过 `elfi-parser` 运行它。
    -   如果解析成功，则打印成功消息并以代码 0 退出。
    -   如果解析失败，则打印详细的错误消息（例如，YAML 解析错误、Tree-sitter 语法错误）并以非零代码退出。

#### `elfi export <PATH>`

解析一个 `.elf` 文件并将其当前状态导出为不同格式。

-   **参数**：
    -   `<PATH>`：`.elf` 文件的路径。
-   **选项**：
    -   `--format <FORMAT>`：输出格式。默认为 `json`。
        -   `json`：将完整的 Automerge 文档导出为单个 JSON 对象。
        -   `raw-json`：以与 Tangle-API 兼容的格式导出文档（参见 `02-core_logic.md`）。
-   **行为**：
    -   将 `.elf` 文件解析为内存中的 Automerge 文档。
    -   将文档序列化为指定格式。
    -   将结果打印到标准输出。

---


# Part III: 示例


## ELF 文件示例

```elf
---
id: block-A
type: markdown
---
# 分析帕尔默群岛企鹅数据

本文档演示了对帕尔默企鹅数据集的分析。我们将加载数据，执行计算，并创建图表。

---
id: block-B
type: markdown
metadata:
  parent: block-A
---
## 1. 数据加载

首先，让我们加载必要的库和数据集。

---
id: block-C
type: code
metadata:
  parent: block-B
  language: python
---
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 加载企鹅数据集
penguins = sns.load_dataset("penguins")

# 显示前几行数据
penguins.head()

---
id: block-D
type: markdown
metadata:
  parent: block-A
---
## 2. 数据可视化

现在，让我们创建一个散点图来可视化鳍状肢长度和喙长度之间的关系，并按物种进行颜色区分。

---
id: block-E
type: code
metadata:
  parent: block-D
  language: python
  interactive: true
---
# 创建散点图
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=penguins,
    x="flipper_length_mm",
    y="bill_length_mm",
    hue="species",
    style="species",
    s=100
)
plt.title("Flipper Length vs. Bill Length by Species")
plt.xlabel("Flipper Length (mm)")
plt.ylabel("Bill Length (mm)")
plt.grid(True)
plt.show()
```

---

